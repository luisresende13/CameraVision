# -*- coding: utf-8 -*-
"""cameras_schedule_threading

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EqyaJCxbDv3OgI94zaPcjrpnVIQNDWmT

### Clone flask api repository
"""

# !git clone --branch my-branch https://github.com/luisresende13/CameraVision

"""### Change directory before installing dependencies"""

# import os
# os.chdir("../")
# import sys
# sys.path.append("../")

"""### Install requirements"""

# !pip install -r requirements.txt
# !pip install lapx

"""# ---"""
"""# Set command-line arguments parser"""

import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="Inference for cameras in parallel")

    # Add arguments
    parser.add_argument('--base_url', type=str, default='ec2', help='Base URL')
    parser.add_argument('--query', type=dict, default={}, help='Query dictionary')
    parser.add_argument('--n_random', type=int, default=None, help='Number of random cameras for inference')
    parser.add_argument('--allow_replacement', action='store_false', help='Allow replacement of cameras in random selection')
    parser.add_argument('--source_field', type=str, default='id', help='Source field for cameras')
    parser.add_argument('--freq_seconds', type=int, default=60, help='Frequency at which to run inference in seconds')
    parser.add_argument('--seconds', type=int, default=None, help='YOLO Seconds parameter')
    parser.add_argument('--execution_seconds', type=int, default=None, help='YOLO Execution seconds parameter')
    parser.add_argument('--log_seconds', type=int, default=None, help='YOLO Log Seconds parameter')
    parser.add_argument('--model', type=str, default='yolov8n.pt', help='Model filename')
    parser.add_argument('--task', type=str, default='track', help='YOLO task')
    parser.add_argument('--device', type=str, default='gpu', help='cpu or gpu device')
    parser.add_argument('--capture', type=str, default='opencv', help='Video source capture method. opencv or yolo')
    parser.add_argument('--process', type=str, default='none', help='Post processing routine to use. One of: none, bigquery, trigger or bigquery-trigger')
    parser.add_argument('--retries', type=int, default=1, help='Maximum number of retries for YOLO failed requests')
    parser.add_argument('--backoff_factor', type=float, default=1.0, help='Backoff factor for retries')
    parser.add_argument('--process_type', type=str, default='threads', help='Process type of the scheduler. threads or process')
    parser.add_argument('--max_running_instances', type=int, default=8, help='Maximum number of concurrent instances of the scheduler')
    parser.add_argument('--thread_delay', type=float, default=2.0, help='Delay between start of concurrent inferences')
    # parser.add_argument('--retry_codes', type=int, nargs='+', default=[500], help='HTTP status codes to retry')
    # parser.add_argument('--raise_on_status', action='store_true', help='Raise exception on non-2xx status codes')
    # parser.add_argument('--show_params', action='store_true', help='Show request parameters')
    # parser.add_argument('--daemon', action='store_true', help='Set daemon attribute for threads')
    
    args = parser.parse_args()
        
    return args


# ---
# Get command-line arguments
args = parse_args()

# ---
# Set main variables

yolo_minutes = args.freq_seconds / 60
max_running_instances = args.max_running_instances
process_type = args.process_type

# Extract parameters from the 'params' dictionary
params = {
    'task': args.task,
    'device': args.device,
    'capture': args.capture,
    'process': args.process,
    'retries': args.retries,
    'retry_delay': args.backoff_factor,
    'log_seconds': args.log_seconds,
    'model': args.model,
}

main_job_params = {
    'base_url': args.base_url,
    'query': args.query,
    'n_random': args.n_random,
    'allow_replacement': args.allow_replacement,
    'source_field': args.source_field,
    'seconds': args.seconds,
    'execution_seconds': args.execution_seconds,
    'thread_delay': args.thread_delay,
    'params': params,
}

# ---
# Set base url

"""### Set envirornment variables"""
import os

"""### Set ec2 instance"""

from modules.aws import EC2Instance

if args.base_url == 'ec2':
    ec2 = EC2Instance(test=True)
    main_job_params['base_url'] = ec2.url
    
"""### --- """    
"""### Function to request parallel inference for cameras in database"""

import time
import datetime
import pytz
import urllib
import requests
import json
import numpy as np
from requests_futures.sessions import FuturesSession
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import threading
from queue import Queue
import cv2
from ultralytics import YOLO

# Set the timezone to Brazil
timezone = pytz.timezone("America/Sao_Paulo")

source_fields_names = {
    "id": "camera_id",
    "url": "source",
}

# Initialize source specific model instances
model_instances = {}

def cameras_inference(
    base_url='https://octa-vision-oayt5ztuxq-ue.a.run.app',
    query={},
    n_random=1,
    allow_replacement=False,
    source_field="id",
    seconds=None,
    execution_seconds=None,
    params={},
    max_retries=1,
    backoff_factor=1,
    retry_codes=[500],
    raise_on_status=False,
    show_params=True,
    process_type='threads',
    daemon=True,
    thread_delay=1,
):

    # Cameras request start
    start_time = time.time()
    start = datetime.datetime.now(timezone).strftime("%Y-%m-%d %H:%M:%S %Z")
    print("\nStart Time (Brazil):", start)

    # Start async session
    session = FuturesSession()

    # Retry configuration
    if max_retries > 1:
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=backoff_factor,
            status_forcelist=retry_codes,
            raise_on_status=raise_on_status,
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount(base_url, adapter)

    # Get cameras from database
    try:
        # res = requests.get(f'{base_url}/cameras')
        res = session.get(f'{base_url}/cameras').result()
        if res.ok:
            cameras = res.json()
        else:
            raise Exception(f'Code {res.status_code}, {res.reason}')
    except Exception as e:
        print(f'\nGET /cameras FAILED: {e}')
        cameras = []

    # Report /camera success
    if len(cameras):
        cameras_time = round(time.time() - start_time, 2)
        print(f'\nGET /cameras SUCCESS. EXECUTION-TIME: {cameras_time} s\n')

    if query:
        # Format query dict argument
        query = {key: value if type(value) is list else [value] for key, value in query.items()}

        # Query cameras based on attribute values
        cameras = [camera for camera in cameras if any([any([value in camera[key] for value in values]) for key, values in query.items()])]

    # Random sample with size `n_random`
    if n_random is not None and len(cameras):
        cameras = np.random.choice(cameras, size=n_random, replace=allow_replacement)

    # Override `seconds` query parameter
    if seconds is not None:
        params['seconds'] = seconds
    elif execution_seconds is not None:
        params['execution_seconds'] = execution_seconds

    # Build requests url and params
    params_list = []
    for camera in cameras:
        camera_id = camera[source_field]  # camera id or source url
        params[source_fields_names[source_field]] = camera_id
        if camera_id not in model_instances:
            model_instances[camera_id] = YOLO(f"models/{params['model']}")
        params_list.append({**params, "model": model_instances[camera_id]})

    # Display list of request parameters
    if show_params:
        print("\nREQUEST LIST:", json.dumps(params_list, indent=4), '\n')

    # --
    # RUN INFERENCE IN PARALLEL

    # Get reference to start time
    inference_start_time = time.time()

    # Create a list of queues to hold the results
    results_queues = [Queue() for _ in params_list]

    # Start/finish threads
    ts = []
    for params_dict, result_queue in zip(params_list, results_queues):
        thread = threading.Thread(target=yolo_watch_camera_sync, args=(params_dict, result_queue), daemon=True)
        thread.start()
        ts.append(thread)
        time.sleep(thread_delay)

    # # Wait for threads to finish
    # for thread in ts:
    #   thread.join()

    # Continuously check for completed threads and retrieve the results as they complete
    completed_count = 0
    total_threads = len(ts)
    results = []

    while completed_count < total_threads:
        for thread, result_queue in zip(ts, results_queues):
            if not thread.is_alive() and not result_queue.empty():
                result = result_queue.get()
                results.append(result)
                completed_count += 1

                # Report progress
                elapsed_time = round(time.time() - inference_start_time, 2)
                result_example = str(result)[:160]
                if isinstance(result, list):
                    result_example = len(result)
                print(f'   Camera {completed_count}/{total_threads} · Elapsed-Time: {elapsed_time} s · Response: {result_example}')

    # Clean up and close windows
    cv2.destroyAllWindows()

    # Start Pool
    # if process_type == 'threads':
    #     # Use parallel execution with threads
    #     executor = ThreadPoolExecutor(max_workers=len(cameras))

    #     # Set the daemon attribute for the underlying threads
    #     if daemon:
    #         for t in executor._threads:
    #             t.daemon = daemon

    # elif process_type == 'process':
    #     # Use parallel execution with processes
    #     executor = ProcessPoolExecutor(max_workers=len(cameras))

    #     # Set the daemon attribute for the underlying processes
    #     if daemon:
    #         for p in executor._processes:
    #             p.daemon = daemon

    # futures = [executor.submit(yolo_watch_camera_sync, args) for args in params_list]

    # Get the actual results from the completed tasks
    # results = []
    # for index, future in enumerate(as_completed(futures)):
    #     # Wait for process to finish
    #     result = future.result()
    #     results.append(result)

    #     # Report progress
    #     elapsed_time = round(time.time() - inference_start_time, 2)
    #     result_example = str(result)[:160]
    #     if isinstance(result, list):
    #         result_example = len(result)
    #     print(f'   Camera {index + 1}/{len(futures)} · Elapsed-Time: {elapsed_time} s · Response: {result_example}')

    # Clean up the executor manually
    # executor.shutdown()

    inference_elapsed_time = round(time.time() - inference_start_time, 2)
    total_elapsed_time = round(time.time() - start_time, 2)
    end = datetime.datetime.now(timezone).strftime("%Y-%m-%d %H:%M:%S %Z")

    print(f"\n* DONE! · CAMERAS: {len(params_list)} · REQUESTED-SECONDS: {seconds} s · REQUESTED-EXECUTION-SECONDS: {execution_seconds} s · INFERENCE-SECONDS: {inference_elapsed_time} · TOTAL-SECONDS: {total_elapsed_time} s")
    print("\nEnd Time (Brazil):", end, '\n')

    return [] # results

"""### Torch settings"""

# import torch.multiprocessing as mp

# # Set multiprocessing start method to 'spawn'
# try:
#     mp.set_start_method('fork')
# except Exception as e:
#     print(f"mp.set_start_method IS ALREADY SET. ERROR: {e}")

# import torch

# Check if a GPU is available
# if torch.cuda.is_available():
#     # Set the default GPU (index 0 in this example)
#     torch.cuda.set_device(0)

"""### Environment settings"""

import os
from modules.yolo_util import yolo_watch_camera

def yolo_watch_camera_sync(kwargs, results_queue):
    results = yolo_watch_camera(**kwargs)
    results = list(results)
    # After getting the result, put it into the results_queue
    results_queue.put(results)
    # return results

"""### Scheduler settings"""

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import time

# Set jobs
# main_job = yolo_job

# Set job frequency
# yolo_minutes = 0.5
yolo_seconds = int(yolo_minutes * 60)

# Schedule the job to run every 30 seconds, starting at 00:00 hours
if yolo_minutes < 1.0:
    yolo_trigger = CronTrigger(second=f'*/{yolo_seconds}')
else:
    yolo_trigger = CronTrigger(minute=f'*/{int(yolo_minutes)}')

print('\nJOB FREQUENCY:', yolo_seconds, 's\n')

"""## Scheduler Main Job

#### Function usage - Run inference for cameras in parallel
"""

# params = {
#     'task': 'track',
#     'device': 'cpu',
#     'capture': 'opencv',
#     'process': 'bigquery',
#     'model': 'yolov8n.pt',
#     'retries': 1,
#     'retry_delay': 1.0,
#     'log_seconds': None,
# }

responses_list = []

def main_job():
    responses = cameras_inference(
        **main_job_params,
        max_retries=1,
        backoff_factor=0.2,
        retry_codes=[500],
        raise_on_status=False,
        show_params=False,
        process_type='threads',
        daemon=True,
    )

    # Gather responses
    responses_list.append(responses)


# ---
# Example usage

# main_job()

"""#### Show list of responses"""

print('responses_list:', responses_list)

"""## Scheduler execution"""

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.executors.pool import ThreadPoolExecutor as TPE, ProcessPoolExecutor as PPE
import time
import datetime
import pytz

# Set the timezone to Brazil
timezone = pytz.timezone("America/Sao_Paulo")

# Set the process type
# process_type = "threads"

# Set the maximum running instances
# max_running_instances = 8

if process_type == 'process':
    # Create a ProcessPoolExecutor instance
    process_executor = PPE(max_workers=max_running_instances)

    # # Access the underlying ThreadPoolExecutor and set the daemon attribute for each thread
    # for p in thread_executor._processes:
    #     p.daemon = True

    # Create a BackgroundScheduler instance with the process executor
    sched = BackgroundScheduler(executors={'processpool': process_executor})

elif process_type == 'threads':
    # Create a ThreadPoolExecutor instance with max_workers set
    thread_executor = TPE(max_workers=max_running_instances)

    # # Access the underlying ThreadPoolExecutor and set the daemon attribute for each thread
    # for thread in thread_executor._threads:
    #     thread.daemon = True

    # Create a BackgroundScheduler instance with the thread executor
    sched = BackgroundScheduler(executors={'threadpool': thread_executor})

# Start the scheduler
sched.start()

# Schedule the main_job to run at the desired frequency
sched.add_job(main_job, yolo_trigger, max_instances=max_running_instances)

try:
    # Report scheduler start
    print("* SCHEDULER STARTED (Brazil):", datetime.datetime.now(timezone).strftime('%Y-%m-%d %H:%M:%S %Z'), '\n\n')
    # Keep the script running (blocked) to allow the scheduled jobs to execute
    while True:
        time.sleep(1)
except (KeyboardInterrupt, SystemExit):
    # Stop the scheduler gracefully on keyboard interrupt
    sched.shutdown()
    print("\n* SCHEDULER STOPPED.")

"""### Run schedule in the background as python script"""

# !nohup python sched.py
# !python cameras_sched.py